# Machine Learning Algorithms Implementation  

## Overview  
This repository contains the implementation of fundamental machine learning algorithms. Each algorithm is presented as a Jupyter Notebook for ease of understanding and experimentation. The focus is on building a strong foundation in machine learning by exploring key concepts, training models, and performing evaluations.

---

## Contents  

1. **[Simple Linear Regression.ipynb](Simple%20Linear%20Regression.ipynb)**  
   - Implements a simple linear regression model for predicting a continuous dependent variable using a single independent variable.  
   - Includes visualization of the regression line and performance metrics like MSE and R-squared.  

2. **[Multiple Linear Regression.ipynb](Multiple%20Linear%20Regression.ipynb)**  
   - Demonstrates multiple linear regression, which uses multiple features to predict a target variable.  
   - Covers feature importance and multicollinearity analysis.  

3. **[Logistic Regression.ipynb](Logistic%20Regression.ipynb)**  
   - Implements logistic regression for binary classification tasks.  
   - Highlights the use of metrics such as accuracy, precision, recall, and ROC-AUC.  

4. **[Decision Tree.ipynb](Decision%20Tree.ipynb)**  
   - Explores decision tree algorithms for classification and regression tasks.  
   - Includes visualization of decision trees and hyperparameter tuning for improved performance.  

5. **[Random Forest Classifier.ipynb](Random%20Forest%20Classifier.ipynb)**  
   - Demonstrates the use of the random forest ensemble method for classification.  
   - Emphasizes feature importance and robustness against overfitting.  

6. **[Naive Bayes.ipynb](Naive%20Bayes.ipynb)**  
   - Covers the Naive Bayes algorithm for classification tasks.  
   - Includes applications in text classification and spam detection.  

7. **[K-Means Clustering.ipynb](K-Means%20Clustering.ipynb)**  
   - Implements K-Means for unsupervised clustering tasks.  
   - Includes visualization of clusters and discussion on choosing the optimal number of clusters using the Elbow Method.  

8. **[SVM + K-fold CV.ipynb](SVM%20+%20K-fold%20CV.ipynb)**  
   - Demonstrates Support Vector Machines (SVM) for classification tasks.  
   - Incorporates K-fold cross-validation for robust model evaluation.  

9. **[paramtuning.ipynb](paramtuning.ipynb)**  
   - Illustrates parameter tuning for various machine learning models.  
   - Covers grid search and random search techniques to optimize hyperparameters.  

---

## Prerequisites  

To run these notebooks, you will need:  
- Python 3.x  
- Libraries: `numpy`, `pandas`, `matplotlib`, `seaborn`, `scikit-learn`, `jupyter`    
